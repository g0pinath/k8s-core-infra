  # Forcing Kubelet metrics scraping on http
  kubelet:
    enabled: true
    serviceMonitor:
      https: true #https://automateddeveloper.blogspot.com/2019/02/kubernetes-prometheus-getting-started.html
      #when its set to false, scraping is on TCP 10250 which has changed apprently, so need to use 10255 using https
  # Disabling scraping of Master Nodes Components
  kubeControllerManager:
    enabled: false
  kubeScheduler:
    enabled: false
  kubeEtcd:
    enabled: false
  kubeProxy:
    enabled: false
  grafana:
    enabled: false
#    plugins:
#   - grafana-piechart-panel
  prometheusOperator:
    enabled: true
    admissionWebhooks:
      enabled: false
    tls:
      enabled: false #if this is not false, the operator will be stuck in container creating status
  prometheus:
    
    prometheusSpec:
      retention: 1h
      storageSpec: 
      ## Using PersistentVolumeClaim
      ##
        volumeClaimTemplate:
          spec:
            storageClassName: default #gp2 - aws ebs
            accessModes: ["ReadWriteOnce"]
            resources:
              requests:
                storage: 10Gi
      podMetadata:
        labels:
          thanos-store-api: "true"
      thanos:
        version: v0.4.0
        resources:
          limits:
            cpu: 500m
            memory: 500Mi
          requests:
            cpu: 100m
            memory: 500Mi
  #For long term storage - the secret is create with values from object-store.yml
        objectStorageConfig:
          key: thanos.yml #this is the name of the file you gave when creating secret | to check use  k describe secret thanos-objstore-config and look at the data value.
          name: thanos-objstore-config           
          #An example is given below
          #$ThanosStorageConfig = ".\k8s-yml-templates\prometheus-grafana\prom-values\object-store.yml"
          #kubectl -n monitoring create secret generic thanos-objstore-config --from-file=thanos.yml=$ThanosStorageConfig          
    #sample rules
  additionalPrometheusRulesMap: 
    rule-name:
      groups:
      - name: custom_alerts_group
        rules:
        - alert: KubeDeploymentReplicasMismatch1m
          expr: (kube_deployment_spec_replicas{job="kube-state-metrics",namespace=~".*"} != kube_deployment_status_replicas_available{job="kube-state-metrics",namespace=~".*"}) and (changes(kube_deployment_status_replicas_updated{job="kube-state-metrics",namespace=~".*"}[1m]) == 0)
          for: 1m
          labels:
            severity: warning
          annotations:
            description: Deployment {{ $labels.namespace }}/{{ $labels.deployment }} has not matched the expected number of replicas for longer than 1 minutes.
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedeploymentreplicasmismatch
            summary: Deployment has not matched the expected number of replicas.
        - alert: KubePodNotReady1m
          expr: sum by(namespace, pod) (max by(namespace, pod) (kube_pod_status_phase{job="kube-state-metrics",namespace=~".*",phase=~"Pending|Unknown"}) * on(namespace, pod) group_left(owner_kind) topk by(namespace, pod) (1, max by(namespace, pod, owner_kind) (kube_pod_owner{owner_kind!="Job"}))) > 0
          for: 1m
          labels:
            severity: warning
          annotations:
            description: Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a non-ready state for longer than 1 minutes.
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepodnotready
            summary: Pod has been in a non-ready state for more than 15 minutes.


  alertmanager:
    #alertmanagerSpec:    
    #  secrets:
    #  - alertmanager-email
    #This secret cant be referred by the below config file. So the email secret is going to be there in the YML file.
    config:
      global:
        resolve_timeout: 5m
        slack_api_url: dummy #$env:K8S_SLACK_NOTIFICATIONS_URL
        #smtp_smarthost: smtp.gmail.com:587
        #smtp_from: 'gopinath.sastra@gmail.com'
        #smtp_auth_username: 'gopinath.sastra@gmail.com'
        #smtp_auth_identity: 'gopinath.sastra@gmail.com'
        #smtp_auth_password: 'DUMMY' #is replaced by helm
      route:
        group_by: ['job']
        group_wait: 30s
        group_interval: 5m
        repeat_interval: 12h
        receiver: 'admin'
        routes:
        - match:
            alertname: Watchdog
          receiver: 'admin'
      receivers:
      - name: admin
        slack_configs:
        - channel: '#alerts'
        # Alertmanager templates apply here.
          title: "{{ range .Alerts }}{{ .Annotations.summary }}\n{{ end }}"
          text: "{{ range .Alerts }}{{ .Annotations.description }}\n{{ end }}"
        #email_configs:
        #- to: 'gopinath.sastra@gmail.com'

          
#Config values are written to /etc/alertmanager/config/alertmanager.yaml file in the alertmanager pod.
#Grafana custom dashboards and imported dashboards are gone when pod is recreated and is not persistent.            
      #externalLabels:
      #  cluster_environment: workshop #use this label if you are using https://observability.thomasriley.co.uk/prometheus/using-thanos/high-availability/